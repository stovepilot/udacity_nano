README: Please add the following sections and corresponding details in the README, this will help you write a wholesome and professional README.

Project description: This should gives a gist of what is to be expected. :x:
Database design: Describe the schema, you should lay down what are the tables (fact and dimension tables) clearly. Mention, the purpose for each of them. :x:
ETL Process: This section describes the processing of the logs and creating different tables so that analytical queries can be run on them. It also describes, which directories have what kind of data and how are you extracting and transforming it. :x:
Project Repository files: This section describes what files are for which purpose in the project :x:
How To Run the Project: This describes the steps to run the project :white_check_mark:
:x: -> indicates that these sections are not yet covered in the README.

I would suggest using proper headings, emphasis, underline the relevant keywords. Use bullet points, add link URLs and images to make the README better.
https://guides.github.com/features/mastering-markdown/

OPTIONAL
To make your project even more appealing on github, it would be great to put an ER diagram, which could help someone looking at the project get direct
understanding into the database implemented. These small things matters during candidate selection. An example for you:

# Sparkify ETL
## Project description 
Learning project for Udacity DE Nanodegree.  The code will built a db in postgress and populate it by reading, wranging logfiles and importing the reulting dataframes into postgress.

## Database design: 
A star schema is used to allow detailed analytical queries to be run against the data.  See also the ER diagram below showing the relationships between the tables,

### songplays
Fact table holding data about each songplay

### songs
Dimension table holding data about each song - with a foreign key to get information about the artist who sang the song

### artists
Dimension table holding data about each artist

### users
Dimension table holding data about the __current status__ of each each user

### time
Dimenstion data holding data about each timestamp

![ER diagram of the schema](./img/sparkifydb_erd.png "ER diagram of the schema")

## ETL Process: 
### Song and artist data import
Song and artist data is imported from json files /home/workspace/data/song_data

For each of song and artist
- Each file is read and the appropriate data appended to a dataframe
- Some limited wrangling is performed on the df (duplicates are removed and some datatypes are changed)
- The df is exported as a csv 
- The csv is imported into the postgres table using the Postgress COPY command
- The csv is dropped

### Log data import
User, time and songplay data is imported from line delimited json files in /home/workspace/data/log_data

- Each file is read and the data appended to a log_data dataframe
- A time df is created, wrangled, exported as a csv and imported into Postgres using COPY 
    - For the time data this also involves the derivation of time features from the timestamp (day_of_week, month, etc)
- A user df is created, wrangled, exported as a csv and imported into Postgres using COPY
- The csv are dropped

### Songplay data import
The songplay data is generated by merging the song and artist data with the log data
- The song and artist data is joined using `song_select` and a dataframe created
- The dataframe is merge with the log_data dataframe created earlier
- The resulting songplay dataframe is wrangled, exported as a csv and imported into Postgres using COPY

## Project Repository files
- `create_tables.py` - Code to drop (if necessary) and build a Postgres db to contain Sparkify song and log data
- `etl.py` - Code to read Sparkify song data and log data from the data directory and and read it into a a postgres database
- `sql_queries.py` - contains the various SQL strings used by the code

## How To Run the Project
1. Run create_tables.py to drop (if necessary) and build a Postgres db to contain Sparkify song and log data
2. Run etl.py to read Sparkify song data and log data from the data directory and and read it into a a postgres database

## Assoc files
- The data folder contains the log and song json files.
- The tmp folder is used to hold the temporary csv files that are copied directly into Postgres.
- The dev folder contains various files used during development.
- The img folder contains images used in the readme.