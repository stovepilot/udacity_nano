{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import configparser\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import configparser\n",
    "import psycopg2\n",
    "import boto3\n",
    "import sql_queries\n",
    "import json\n",
    "\n",
    "import importlib\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_iam_role():\n",
    "    try:\n",
    "        print(\"1.1 Creating a new IAM Role\") \n",
    "        dwhRole = iam.create_role(\n",
    "            Path='/',\n",
    "            RoleName=IAM_ROLE_NAME,\n",
    "            Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "            AssumeRolePolicyDocument=json.dumps(\n",
    "                {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "                   'Effect': 'Allow',\n",
    "                   'Principal': {'Service': 'rexdshift.amazonaws.com'}}],\n",
    "                 'Version': '2012-10-17'})\n",
    "        )    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    print(\"1.2 Attaching Policy\")\n",
    "\n",
    "    iam.attach_role_policy(RoleName=IAM_ROLE_NAME,\n",
    "                           PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                          )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "    print(\"1.3 Get the IAM role ARN\")\n",
    "    return iam.get_role(RoleName=IAM_ROLE_NAME)['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_new_cluster():\n",
    "    print(\"Creating a new Redshift cluster\")\n",
    "\n",
    "    try:\n",
    "        response = redshift.create_cluster(        \n",
    "            #HW\n",
    "            ClusterType=DB_CLUSTER_TYPE,\n",
    "            NodeType=DB_NODE_TYPE,\n",
    "            NumberOfNodes=int(DB_NUM_NODES),\n",
    "            \n",
    "            #Snapshots\n",
    "            AutomatedSnapshotRetentionPeriod=DB_SNAPSHOT_RETENTION,\n",
    "\n",
    "            #Identifiers & Credentials\n",
    "            DBName=DB_NAME,\n",
    "            ClusterIdentifier=DB_CLUSTER_IDENTIFIER,\n",
    "            MasterUsername=DB_USER,\n",
    "            MasterUserPassword=DB_PASSWORD,\n",
    "\n",
    "            #Roles (for s3 access)\n",
    "            IamRoles=[roleArn]\n",
    "        )\n",
    "        \n",
    "        wait_for_cluster('available')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pause_cluster():\n",
    "    \n",
    "    print(f'Pausing cluster {DB_CLUSTER_IDENTIFIER}')\n",
    "    print(f'Deleting old snapshot ({DB_SNAPSHOT_IDENTIFIER})')\n",
    "    try:\n",
    "        delete_snapshots(DB_SNAPSHOT_IDENTIFIER)\n",
    "        print(f'Deleting cluster {DB_CLUSTER_IDENTIFIER} while retaining snapshot ({DB_SNAPSHOT_IDENTIFIER})')\n",
    "        redshift.delete_cluster( ClusterIdentifier=DB_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=False, FinalClusterSnapshotIdentifier=DB_SNAPSHOT_IDENTIFIER)\n",
    "        wait_for_cluster('deleted')\n",
    "        print(f'Cluster deleted with snapshot {DB_SNAPSHOT_IDENTIFIER} retained.')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def resume_cluster():\n",
    "    print(f'Resuming cluster from snapshot {DB_SNAPSHOT_IDENTIFIER}')\n",
    "    redshift.restore_from_cluster_snapshot( ClusterIdentifier=DB_CLUSTER_IDENTIFIER, SnapshotIdentifier=DB_SNAPSHOT_IDENTIFIER, IamRoles=[roleArn]  )\n",
    "    \n",
    "    wait_for_cluster('available')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def delete_snapshots(snapshot_identifier=None):\n",
    "    count_snapshots=len(redshift.describe_cluster_snapshots(ClusterIdentifier=DB_CLUSTER_IDENTIFIER)['Snapshots'])\n",
    "    if count_snapshots > 0:\n",
    "        print(f\"Found {count_snapshots} snapshot(s):\")\n",
    "        try:\n",
    "            if snapshot_identifier != None:\n",
    "                if check_for_snapshot():\n",
    "                    print(f'Deleting snapshot {snapshot_identifier}')\n",
    "                    redshift.delete_cluster_snapshot(SnapshotIdentifier=snapshot_identifier)\n",
    "                else:\n",
    "                    print(f'Snapshot {snapshot_identifier} not present - continuing')\n",
    "            else:\n",
    "                \n",
    "                for snapshot in redshift.describe_cluster_snapshots(ClusterIdentifier=DB_CLUSTER_IDENTIFIER)['Snapshots']:\n",
    "                    if snapshot['SnapshotType']=='manual':\n",
    "                        print(f\"Deleting snapshot {snapshot['SnapshotIdentifier']}\")\n",
    "                        redshift.delete_cluster_snapshot(SnapshotIdentifier=snapshot['SnapshotIdentifier'])\n",
    "                    elif snapshot['SnapshotType']=='automated':\n",
    "                        print(f\"Automated snapshot {snapshot['SnapshotIdentifier']} cannot be manually deleted but will be droped after {DB_SNAPSHOT_RETENTION} day retention period\")\n",
    "                    else:\n",
    "                        print(f\"Found unknown snapshot {snapshot['SnapshotIdentifier']} - taking no action\")\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def delete_cluster():\n",
    "    print(f'Deleting {DB_CLUSTER_IDENTIFIER}')\n",
    "    redshift.delete_cluster( ClusterIdentifier=DB_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "    wait_for_cluster('deleted')\n",
    "    delete_snapshots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def wait_for_cluster(desired_status):\n",
    "\n",
    "    i=0\n",
    "    try:\n",
    "        while redshift.describe_clusters(ClusterIdentifier=DB_CLUSTER_IDENTIFIER)['Clusters'][0]['ClusterStatus']!=desired_status:\n",
    "            print (f\"Waiting for cluster to be {desired_status} (status currently \\'{redshift.describe_clusters(ClusterIdentifier=DB_CLUSTER_IDENTIFIER)['Clusters'][0]['ClusterStatus']}\\')\")\n",
    "            time.sleep(10)\n",
    "            i += 1\n",
    "\n",
    "            if i > 30:\n",
    "        #       Error\n",
    "                print(f'Error - cluster not {desired_status} after 5 minutes')\n",
    "                exit(1)\n",
    "\n",
    "        # TODO - handle error better    \n",
    "        print(f'Cluster {desired_status}')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    - Creates the tables\n",
    "    \"\"\"\n",
    "    print('Creating tables')\n",
    "    for query in sql_queries.create_table_queries:\n",
    "        cur.execute(query.format(DB_ROLE_ARN))\n",
    "        conn.commit()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def copy_to_dim(cur, conn):\n",
    "    \"\"\"\n",
    "    - populates the dimension tables\n",
    "    \"\"\"\n",
    "    print('Populating dimension tables')\n",
    "    for query in sql_queries.copy_to_dim_queries:\n",
    "        print(query.format(DB_ROLE_ARN))\n",
    "        cur.execute(query.format(DB_ROLE_ARN))\n",
    "        conn.commit()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def copy_to_fact(cur, conn):\n",
    "    \"\"\"\n",
    "    - populates the fact tables\n",
    "    \"\"\"\n",
    "    print('Populating fact tables')\n",
    "    for query in sql_queries.copy_to_fact_queries:\n",
    "        cur.execute(query.format(DB_ROLE_ARN))\n",
    "        conn.commit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def drop_tables(cur, conn):\n",
    "    \"\"\"\n",
    "    - drops the dimension tables\n",
    "    \"\"\"\n",
    "    print('Dropping tables')\n",
    "    for query in sql_queries.drop_dim_table_queries:\n",
    "        print(query.format(DB_ROLE_ARN))\n",
    "        cur.execute(query.format(DB_ROLE_ARN))\n",
    "        conn.commit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_for_snapshot():\n",
    "    try:\n",
    "        snapshot_exists = DB_SNAPSHOT_IDENTIFIER in [snapshot['SnapshotIdentifier'] for snapshot in redshift.describe_cluster_snapshots(SnapshotIdentifier=DB_SNAPSHOT_IDENTIFIER)['Snapshots']]\n",
    "        return snapshot_exists\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "DB_SNAPSHOT_IDENTIFIER = 'dwh-snapshot'\n",
    "DB_SNAPSHOT_RETENTION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('aws.cfg'))\n",
    "\n",
    "DB_CLUSTER_TYPE        = config.get(\"CLUSTER\",\"DB_CLUSTER_TYPE\")\n",
    "DB_NUM_NODES           = config.get(\"CLUSTER\",\"DB_NUM_NODES\")\n",
    "DB_NODE_TYPE           = config.get(\"CLUSTER\",\"DB_NODE_TYPE\")\n",
    "\n",
    "DB_CLUSTER_IDENTIFIER  = config.get(\"CLUSTER\",\"DB_CLUSTER_IDENTIFIER\")\n",
    "DB_NAME                = config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DB_USER                = config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DB_PASSWORD         = config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DB_PORT                = config.get(\"CLUSTER\",\"DB_PORT\")\n",
    "\n",
    "IAM_ROLE_NAME          = config.get(\"IAM_ROLE\", \"IAM_ROLE_NAME\")\n",
    "ARN                    = config.get(\"IAM_ROLE\", \"ARN\")\n",
    "\n",
    "(DB_USER, DB_PASSWORD, DB_NAME)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DB_CLUSTER_TYPE\", \"DB_NUM_NODES\", \"DB_NODE_TYPE\", \"DB_CLUSTER_IDENTIFIER\", \n",
    "                   \"DB_NAME\", \"DB_USER\", \"DB_PASSWORD\", \"DB_PORT\", \"IAM_ROLE_NAME\",\"ARN\"],\n",
    "              \"Value\":\n",
    "                  [DB_CLUSTER_TYPE, DB_NUM_NODES, DB_NODE_TYPE, DB_CLUSTER_IDENTIFIER, \n",
    "                   DB_NAME, DB_USER, DB_PASSWORD, DB_PORT, IAM_ROLE_NAME, ARN],\n",
    "             })\n",
    "\n",
    "config.read_file(open('aws.credentials'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "\n",
    "DB_ENDPOINT = ''\n",
    "DB_ROLE_ARN = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create clients for IAM, EC2 and Redshift\n",
    "**Note**: We are creating these resources in the the **us-west-2** region. Choose the same region in the your AWS web console to the see these resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )\n",
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )\n",
    "\n",
    "roleArn=create_iam_role()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 Creating a new IAM Role\n",
      "An error occurred (EntityAlreadyExists) when calling the CreateRole operation: Role with name dwhRole already exists.\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::109203719027:role/dwhRole\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### - Create a [RedShift Cluster](https://console.aws.amazon.com/redshiftv2/home)\n",
    "#### - For complete arguments to `create_cluster`, see [docs](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/redshift.html#Redshift.Client.create_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming cluster operation based on snapshot dwh-snapshot\n"
     ]
    }
   ],
   "source": [
    "# resume a saved cluster or create a new one\n",
    "if check_for_snapshot()==True:\n",
    "    print(f'Resuming cluster operation based on snapshot {DB_SNAPSHOT_IDENTIFIER}')\n",
    "#    resume_cluster()\n",
    "else:\n",
    "    print(f'No saved instances of {DB_CLUSTER_IDENTIFIER} found - creating a new cluster')\n",
    "#    create_new_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\"\n",
    "                        .format(DB_ENDPOINT, DB_NAME, DB_USER, DB_PASSWORD, DB_PORT))\n",
    "\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating staging tables\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(sql_queries)\n",
    "create_tables(cur, conn)\n",
    "copy_to_dim(cur, conn)\n",
    "copy_to_fact(cur, conn)\n",
    "# drop_tables(cur, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pausing cluster dwhCluster\n",
      "Deleting old snapshot (dwh-snapshot)\n",
      "Deleting snaphot dwh-snapshot\n",
      "local variable 'snapshot' referenced before assignment\n",
      "Deleting cluster dwhCluster while retaining snapshot (dwh-snapshot)\n",
      "An error occurred (ClusterSnapshotAlreadyExists) when calling the DeleteCluster operation: Cannot create the snapshot because a snapshot with the identifier dwh-snapshot already exists.\n"
     ]
    }
   ],
   "source": [
    "pause_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming cluster from snapshot dwh-snapshot\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Waiting for cluster to be available (status currently 'creating')\n",
      "Cluster available\n"
     ]
    }
   ],
   "source": [
    "resume_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting dwhCluster\n",
      "Waiting for cluster to be deleted (status currently 'deleting')\n",
      "Waiting for cluster to be deleted (status currently 'deleting')\n",
      "Waiting for cluster to be deleted (status currently 'deleting')\n",
      "Waiting for cluster to be deleted (status currently 'deleting')\n",
      "Waiting for cluster to be deleted (status currently 'deleting')\n",
      "Waiting for cluster to be deleted (status currently 'deleting')\n",
      "Waiting for cluster to be deleted (status currently 'deleting')\n",
      "Waiting for cluster to be deleted (status currently 'deleting')\n",
      "An error occurred (ClusterNotFound) when calling the DescribeClusters operation: Cluster dwhcluster not found.\n",
      "Found 1 snapshot(s):\n",
      "Deleting snapshot dwh-snapshot\n"
     ]
    }
   ],
   "source": [
    "delete_cluster()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
