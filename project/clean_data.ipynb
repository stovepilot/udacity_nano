{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import unix_timestamp, udf, lit, explode, split, regexp_extract, col, isnan, isnull, desc, when, sum, to_date, desc, regexp_replace, count, to_timestamp, current_timestamp\n",
    "from pyspark.sql.types import IntegerType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> p { max-width:90% !important; } h1 {font-size:2rem!important } h2 {font-size:1.6rem!important } \n",
       "h3 {font-size:1.4rem!important } h4 {font-size:1.3rem!important }h5 {font-size:1.2rem!important }h6 {font-size:1.1rem!important }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#setting visualization options\n",
    "# https://www.1week4.com/it/machine-learning/udacity-data-engineering-capstone-project/\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)  \n",
    "\n",
    "# modify visualization of the notebook, for easier view\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"\"\"<style> p { max-width:90% !important; } h1 {font-size:2rem!important } h2 {font-size:1.6rem!important } \n",
    "h3 {font-size:1.4rem!important } h4 {font-size:1.3rem!important }h5 {font-size:1.2rem!important }h6 {font-size:1.1rem!important }</style>\"\"\"))# Do all imports and installs here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    This function creates a Spark Sesson and includes necessary Jar and adoop packages in the configuration. \n",
    "    \"\"\"\n",
    "    spark=SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\") \\\n",
    "    .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# from https://knowledge.udacity.com/questions/66798\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import types as T\n",
    "def convert_datetime(x):\n",
    "    try:\n",
    "        start = datetime(1960, 1, 1)\n",
    "        return start + timedelta(days=int(x))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def cleanImmigrationFactData(path_to_files, output_path):\n",
    "    \"\"\"\n",
    "        Description:\n",
    "\n",
    "        Usage:\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    filelist = os.listdir(path_to_files)\n",
    "    \n",
    "    print(f\"The dataset contains {len(filelist)} files\")\n",
    "    \n",
    "    for file in filelist:\n",
    "        \n",
    "        filepath = '{}{}'.format(path_to_files, file)\n",
    "        \n",
    "        size = os.path.getsize('{}/{}'.format(path_to_files, file))        \n",
    "        print(f'Processing {filepath} - dim(bytes): {size} ')\n",
    "        \n",
    "        df_I94 = spark.read.format('com.github.saurfang.sas.spark').load(filepath).persist()\n",
    "\n",
    "        # Snippet taken from https://www.1week4.com/it/machine-learning/udacity-data-engineering-capstone-project/\n",
    "        toInt = udf(lambda x: int(x) if x!=None else x, IntegerType())\n",
    "\n",
    "        for colname, coltype in df_I94.dtypes:\n",
    "            if coltype == 'double':\n",
    "                df_I94 = df_I94.withColumn(colname, toInt(colname))\n",
    "        \n",
    "        # Convert strings to dates\n",
    "        df_I94 = df_I94.withColumn('dtaddto',to_date(col(\"dtaddto\"),\"MMddyyyy\")) \\\n",
    "        .withColumn('dtaddto',to_date(col(\"dtaddto\"),\"MMddyyyy\"))\n",
    "        \n",
    "        # Convert SAS date to dates\n",
    "        udf_datetime_from_sas = udf(lambda x: convert_datetime(x), T.DateType())\n",
    "        \n",
    "        df_I94 = df_I94.withColumn(\"arrdate\", udf_datetime_from_sas(\"arrdate\")) \\\n",
    "        .withColumn(\"depdate\", udf_datetime_from_sas(\"depdate\")) \n",
    "        \n",
    "        # write data out\n",
    "        print(f'Exporting cleaned file to {output_path}')\n",
    "        df_I94.write.format('parquet').mode('overwrite').partitionBy('i94yr','i94mon').save(output_path)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "I94_DATASET_PATH = '../../../../../data/18-83510-I94-Data-2016/'\n",
    "CLEAN_DATA_DIR='data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 12 files\n",
      "Processing ../../../../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat - dim(bytes): 471990272 \n",
      "Exporting cleaned file to data/imm\n",
      "Processing ../../../../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat - dim(bytes): 569180160 \n",
      "Exporting cleaned file to data/imm\n",
      "Processing ../../../../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat - dim(bytes): 444334080 \n",
      "Exporting cleaned file to data/imm\n",
      "Processing ../../../../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat - dim(bytes): 481296384 \n",
      "Exporting cleaned file to data/imm\n",
      "Processing ../../../../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat - dim(bytes): 716570624 \n",
      "Exporting cleaned file to data/imm\n",
      "Processing ../../../../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat - dim(bytes): 625541120 \n",
      "Exporting cleaned file to data/imm\n",
      "Processing ../../../../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat - dim(bytes): 525008896 \n",
      "Exporting cleaned file to data/imm\n",
      "Processing ../../../../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat - dim(bytes): 434176000 \n",
      "Exporting cleaned file to data/imm\n",
      "Processing ../../../../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat - dim(bytes): 556269568 \n",
      "Exporting cleaned file to data/imm\n",
      "Processing ../../../../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat - dim(bytes): 650117120 \n",
      "Exporting cleaned file to data/imm\n",
      "Processing ../../../../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat - dim(bytes): 391905280 \n",
      "Exporting cleaned file to data/imm\n",
      "Processing ../../../../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat - dim(bytes): 523304960 \n",
      "Exporting cleaned file to data/imm\n"
     ]
    }
   ],
   "source": [
    "cleanImmigrationFactData(I94_DATASET_PATH,'{}imm'.format(CLEAN_DATA_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### clean_dimension_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def readDimensionData(label_file):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    print(f'Reading {label_file}')\n",
    "    df_label_full = spark.read.text(label_file, wholetext=True)\n",
    "\n",
    "    return df_label_full\n",
    "\n",
    "def extractAirportCodes(df_label_full):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'Extracting airport codes from dataframe')\n",
    "    # airport codes\n",
    "    pattern='(\\$i94prtl)([^;]+)'\n",
    "\n",
    "    df_extract = df_label_full.withColumn('I94PORT', regexp_extract(col('value'),pattern,2))\n",
    "    df_extract = df_extract.withColumn('port',explode(split('I94PORT','[\\r\\n]+'))).drop('value').drop('I94PORT')\n",
    "    df_I94PORT = df_extract.withColumn('port_code',regexp_extract(col('port'),\"(?<=')[0-9A-Z. ]+(?=')\",0)) \\\n",
    "        .withColumn('city_state',regexp_extract(col('port'),\"(=\\t')([0-9A-Za-z ,\\-()\\/\\.#&]+)(')\",2)) \\\n",
    "        .withColumn('city', split(col('city_state'),',').getItem(0)) \\\n",
    "        .withColumn('state', split(col('city_state'),',').getItem(1)) \\\n",
    "        .withColumn('state', regexp_replace(col('state'), ' *$', '')) \\\n",
    "        .where(col('port')!='') \\\n",
    "        .drop('port') \\\n",
    "    \n",
    "    return df_I94PORT\n",
    "\n",
    "def extractCountryCodes(df_label_full):    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'Extracting country codes from dataframe')\n",
    "    pattern='(i94cntyl)([^;]+)'\n",
    "\n",
    "    df_extract = df_label_full.withColumn('I94RES', regexp_extract(col('value'),pattern,2))\n",
    "    df_extract = df_extract.withColumn('raw',explode(split('I94RES','[\\r\\n]+'))).drop('value').drop('I94RES')\n",
    "    df_I94RES = df_extract.withColumn('country_code',regexp_extract(col('raw'),\"[0-9]+\",0)) \\\n",
    "    .withColumn('country',regexp_extract(col('raw'),\"\\'([A-Za-z ,\\-()0-9]+)\\'\",1)) \\\n",
    "    .where(col('raw')!='') \\\n",
    "    .drop('raw')\n",
    "    \n",
    "    return df_I94RES\n",
    "\n",
    "def extractStateCodes(df_label_full):        \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(f'Extracting state codes from dataframe')\n",
    "    pattern='(i94addrl)([^;]+)'\n",
    "\n",
    "    df_extract = df_label_full.withColumn('i94addrl', regexp_extract(col('value'),pattern,2))\n",
    "    df_extract = df_extract.withColumn('raw',explode(split('i94addrl','[\\r\\n]+'))).drop('value').drop('i94addrl')\n",
    "    df_I94ADDR = df_extract.withColumn('state_code',regexp_extract(col('raw'),\"(?<=')[0-9A-Z. ]+(?=')\",0)) \\\n",
    "    .withColumn('state',regexp_extract(col('raw'),\"(=\\s*\\')([A-Z]+)(\\')\",2)) \\\n",
    "    .where(col('raw')!='') \\\n",
    "    .drop('raw')\n",
    "    \n",
    "    return df_I94ADDR\n",
    "\n",
    "def buildVisaData():    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print('Building visa code df')\n",
    "    columns = ['I94VISA', 'category']\n",
    "    vals = [(1,'Business'),(2,'Pleasure'),(3,'Student')]\n",
    "\n",
    "    df_I94VISA = spark.createDataFrame(vals, columns)\n",
    "    \n",
    "    return df_I94VISA\n",
    "\n",
    "def buildModeData():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print('Building entry mode code df')    \n",
    "    columns = ['I94MODE', 'category']\n",
    "    vals = [(1,'Air'),(2,'Sea'),(3,'Land'),(4,'Not reported')]\n",
    "\n",
    "    df_I94MODE = spark.createDataFrame(vals, columns)\n",
    "    \n",
    "    return df_I94MODE\n",
    "\n",
    "def writeDataFrame(df_output, output_path, output_filename):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    print('Writing data to {}/{}'.format(output_path,output_filename)) \n",
    "    os.makedirs(output_path, exist_ok=True)  \n",
    "    df_output.toPandas().to_csv('{}/{}'.format(output_path,output_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading I94_SAS_Labels_Descriptions.SAS\n",
      "Extracting airport codes from dataframe\n",
      "Writing data to data/dim/I94PORT.csv\n",
      "Extracting country codes from dataframe\n",
      "Writing data to data/dim/I94RES.csv\n",
      "Extracting state codes from dataframe\n",
      "Writing data to data/dim/I94ADDR.csv\n",
      "Building visa code df\n",
      "Writing data to data/dim/I94VISA.csv\n",
      "Building entry mode code df\n",
      "Writing data to data/dim/I94MODE.csv\n"
     ]
    }
   ],
   "source": [
    "I94_LABELS = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "\n",
    "df_labels = readDimensionData(I94_LABELS)\n",
    "\n",
    "df_I94PORT = extractAirportCodes(df_labels)\n",
    "writeDataFrame(df_I94PORT,'{}dim'.format(CLEAN_DATA_DIR),'I94PORT.csv')\n",
    "\n",
    "df_I94RES = extractCountryCodes(df_labels)\n",
    "writeDataFrame(df_I94PORT,'{}dim'.format(CLEAN_DATA_DIR),'I94RES.csv')\n",
    "\n",
    "df_I94ADDR = extractStateCodes(df_labels)\n",
    "writeDataFrame(df_I94PORT,'{}dim'.format(CLEAN_DATA_DIR),'I94ADDR.csv')\n",
    "\n",
    "df_I94VISA = buildVisaData()\n",
    "writeDataFrame(df_I94PORT,'{}dim'.format(CLEAN_DATA_DIR),'I94VISA.csv')\n",
    "\n",
    "df_I94MODE = buildModeData()\n",
    "writeDataFrame(df_I94PORT,'{}dim'.format(CLEAN_DATA_DIR),'I94MODE.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
